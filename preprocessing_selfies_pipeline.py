# -*- coding: utf-8 -*-
"""preprocessing Selfies pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJ763UhgV8_QKxLP6_pHfKOdj9bU7qNX
"""

# -------------------------------------------------------------
# Drug SELFIES Encoding Pipeline
# -------------------------------------------------------------

import selfies as sf
import numpy as np
import time
from pubchempy import get_compounds, Compound
import pandas as pd


# -------------------------------------------------------------
# Load Drug List from Dataset
# -------------------------------------------------------------
def get_list_of_drugs(path_dataset, dataset):
    data = pd.read_csv(path_dataset + dataset + '/drug_protein.csv')
    d1 = data.iloc[:, 0].unique()

    data = pd.read_csv(path_dataset + dataset + '/drug_combinations.csv')
    d2 = data.iloc[:, 3].unique()
    d3 = data.iloc[:, 4].unique()

    drugs = np.unique(np.concatenate((d1, d2, d3)))
    return drugs


# -------------------------------------------------------------
# Settings
# -------------------------------------------------------------
path_dataset = 'path_dataset'
dataset = 'DrugCombDB'
max_len = 200


# -------------------------------------------------------------
# Tokenization + Numeric Mapping Functions
# -------------------------------------------------------------
def tokenize_selfies(s):
    """Split SELFIES string into token units."""
    return sf.split_selfies(s)


def selfies_to_ids(tokens, token_dict, max_len):
    """Convert tokenized SELFIES to integer array using label-encoding."""
    out = np.zeros(max_len, dtype=int)
    for i, tok in enumerate(tokens[:max_len]):
        out[i] = token_dict.get(tok, 0)
    return out


# -------------------------------------------------------------
# SELFIES Extraction
# -------------------------------------------------------------
drug_list = get_list_of_drugs(path_dataset, dataset)
selfies_raw = []

for idx, drug in enumerate(drug_list):
    print(f"[{idx+1}/{len(drug_list)}] Processing: {drug}")

    result = get_compounds(drug, "name")
    if len(result) == 0:
        print("No PubChem match")
        continue

    smiles = Compound.from_cid(result[0].cid).canonical_smiles
    selfies = sf.encoder(smiles)
    tokens = tokenize_selfies(selfies)
    selfies_raw.append(tokens)

    time.sleep(0.3)   # request throttling


# -------------------------------------------------------------
# Vocabulary Construction
# -------------------------------------------------------------
vocab = sorted({tok for row in selfies_raw for tok in row})
token_dict = {tok: i+1 for i, tok in enumerate(vocab)}
print("Vocabulary size =", len(vocab))


# -------------------------------------------------------------
# Convert all SELFIES to numeric ID representation
# -------------------------------------------------------------
encoded = np.array([selfies_to_ids(x, token_dict, max_len) for x in selfies_raw])


# -------------------------------------------------------------
# Save Outputs
# -------------------------------------------------------------
np.save(path_dataset + dataset + '/selfies_raw.npy', np.array(selfies_raw, dtype=object))
np.save(path_dataset + dataset + '/selfies.npy', encoded)
np.save(path_dataset + dataset + '/selfies_vocab.npy', np.array(vocab, dtype=object))

print("\nSaved files:")
print(" - selfies_raw.npy     (tokenized SELFIES)")
print(" - selfies.npy         (label-encoded selfies)")
print(" - selfies_vocab.npy   (token dictionary)")
print("Encoding Shape:", encoded.shape)