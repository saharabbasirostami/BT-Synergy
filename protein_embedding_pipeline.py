# -*- coding: utf-8 -*-
"""protein_embedding_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P-bX4_BdrMvGQgCCGKxQtkraeoF1M7UA
"""

# ============================================
# 0) Install Required Libraries
# ============================================
!pip install biopython tape_proteins torch requests pandas numpy

# ============================================
# 1) Import Libraries
# ============================================
import os
import pandas as pd
import numpy as np
import requests
import concurrent.futures

from Bio import ExPASy, SwissProt
from tape import ProteinBertModel, TAPETokenizer

import torch
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast

BASE = "/content/drive/MyDrive/BT-Synergy/dataset/DrugCombDB"

cell_df = pd.read_excel(f"{BASE}/cell_protein.xlsx")
drug_df = pd.read_excel(f"{BASE}/drug_protein.xlsx")

proteins = set(cell_df["protein"]).union(set(drug_df["protein"]))
proteins = list(proteins)

print("Total unique proteins extracted:", len(proteins))
def update_protein_id(pid):
    url = f"https://rest.uniprot.org/uniprotkb/search?query={pid}&format=json"
    try:
        r = requests.get(url)
        if r.status_code == 200:
            js = r.json()
            if js.get("results"):
                return js["results"][0]["primaryAccession"]
        return None
    except:
        return None


def update_ids_parallel(protein_list):
    updated = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:
        futures = {ex.submit(update_protein_id, p): p for p in protein_list}
        for future in concurrent.futures.as_completed(futures):
            old = futures[future]
            new = future.result()
            if new:
                updated[old] = new
    return updated


updated_ids = update_ids_parallel(proteins)

pd.DataFrame(updated_ids.items(), columns=["old_id", "new_id"]).to_csv(
    f"{BASE}/updated_proteins.csv", index=False
)

print("Updated protein IDs:", len(updated_ids))

df_up = pd.read_csv(f"{BASE}/updated_proteins.csv")
id_map = dict(zip(df_up["old_id"], df_up["new_id"]))

def fetch_sequence(pid):
    try:
        handle = ExPASy.get_sprot_raw(pid)
        rec = SwissProt.read(handle)
        return rec.sequence
    except:
        return None


def fetch_seqs_parallel(id_map):
    seqs = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:
        futures = {ex.submit(fetch_sequence, new): new for new in id_map.values()}
        for future in concurrent.futures.as_completed(futures):
            acc = futures[future]
            seq = future.result()
            if seq:
                seqs[acc] = seq
    return seqs


sequences = fetch_seqs_parallel(id_map)

print("Total fetched sequences:", len(sequences))

pd.DataFrame(sequences.items(), columns=["protein", "sequence"]).to_csv(
    f"{BASE}/protein_sequences.csv", index=False
)


seq_df = pd.read_csv(f"{BASE}/protein_sequences.csv")
sequences = dict(zip(seq_df["protein"], seq_df["sequence"]))

MAX_LEN = 1000
PAD = "X"

sequences = {pid: seq[:MAX_LEN].ljust(MAX_LEN, PAD) for pid, seq in sequences.items()}

print("Final sequence count:", len(sequences))


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

class ProteinDataset(Dataset):
    def __init__(self, seq_dict, tokenizer):
        self.items = list(seq_dict.items())
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        pid, seq = self.items[idx]
        tokens = torch.tensor(self.tokenizer.encode(seq))
        return pid, tokens


def load_model():
    model = ProteinBertModel.from_pretrained("bert-base")
    tok = TAPETokenizer(vocab="iupac")
    return model.to(device), tok

model, tokenizer = load_model()


def get_embeddings(seq_dict, batch_size=16):
    ds = ProteinDataset(seq_dict, tokenizer)
    dl = DataLoader(ds, batch_size=batch_size, collate_fn=lambda x: x)

    embeddings = {}

    for batch in dl:
        names, toks = zip(*batch)
        toks = torch.nn.utils.rnn.pad_sequence(toks, batch_first=True).to(device)

        with torch.no_grad():
            with autocast():
                out = model(toks)[0]            # (B, L, H)
                pooled = out.mean(dim=1).cpu().numpy()

        for i, pid in enumerate(names):
            embeddings[pid] = pooled[i]

    return embeddings


emb = get_embeddings(sequences, batch_size=16)

print("Embedding count:", len(emb))

emb_matrix = np.vstack(list(emb.values()))
np.save(f"{BASE}/protein_embeddings.npy", emb_matrix)

print("Saved to:", f"{BASE}/protein_embeddings.npy")